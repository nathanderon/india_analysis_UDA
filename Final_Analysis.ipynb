{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import codecs\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "from matplotlib_venn import venn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code borrowed and adapted from George Chen, Carnegie Mellon University#\n",
    "#Define function to remove punctuation and whitespace, and lowercase all text\n",
    "def makeWordList(str_object):\n",
    "    \n",
    "    corpus_text = str(str_object)\n",
    "    \n",
    "    for c in string.punctuation:\n",
    "        corpus_text = corpus_text.replace(c, \"\")  # -- (1)\n",
    "    \n",
    "    text = re.sub(r'\\S*\\d\\S*','',corpus_text) # -- (2)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)         # -- (3)\n",
    "    \n",
    "    text = text.lower().split()           # -- (4)         \n",
    "    \n",
    "    li = []\n",
    "    for token in text:\n",
    "        li.append(token)\n",
    "\n",
    "    return \" \".join(li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create single parliament_qs dataframe with all question data\n",
    "li = []\n",
    "\n",
    "for file in glob.glob('./Parliament_Qs/rajyasabha_questions_and_answers_*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "parliament_qs = pd.concat(li, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process parliamentary answers\n",
    "processed_answers = []\n",
    "\n",
    "for str_object in list(parliament_qs[\"answer\"]):\n",
    "    processed_answers.append(makeWordList(str_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import headlines dataset\n",
    "li = []\n",
    "\n",
    "for file in glob.glob('./india_headlines_data/*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "headlines_raw = pd.concat(li, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process headlines, delete headlines object, sample 10% of processed headlines\n",
    "import random\n",
    "processed_headlines = []\n",
    "random.seed(42)\n",
    "headlines = random.sample(list(headlines_raw[\"headline_text\"]), round(len(headlines_raw)/10))\n",
    "                          \n",
    "for str_object in headlines:\n",
    "    processed_headlines.append(makeWordList(str_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import religious headline pre- and post-election datasets\n",
    "li = []\n",
    "\n",
    "for file in glob.glob('./india_headlines_data_pre/religious*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "religious_headlines_pre = pd.concat(li, axis = 0, ignore_index = True)\n",
    "\n",
    "li = []\n",
    "\n",
    "for file in glob.glob('./india_headlines_data_post/religious*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "religious_headlines_post = pd.concat(li, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process religious headlines\n",
    "processed_religious_headlines_pre = []\n",
    "\n",
    "for str_object in list(religious_headlines_pre[\"headline_text\"]):\n",
    "    processed_religious_headlines_pre.append(makeWordList(str_object))\n",
    "    \n",
    "processed_religious_headlines_post = []\n",
    "\n",
    "for str_object in list(religious_headlines_post[\"headline_text\"]):\n",
    "    processed_religious_headlines_post.append(makeWordList(str_object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Description and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Parliament Q&A dataset, aggregating by month and producing a plot of the date distribution\n",
    "answer_dates = pd.DataFrame(pd.to_datetime(parliament_qs[\"answer_date\"]))\n",
    "final_answer_dates = []\n",
    "for date in answer_dates[\"answer_date\"]:\n",
    "    if isinstance(date, datetime.date):\n",
    "        final_answer_dates.append([date, 1])\n",
    "final_answer_dates = pd.DataFrame(final_answer_dates)\n",
    "#final_answer_dates.reset_index().set_index(0)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "agg = final_answer_dates.resample('M', on=0).count()\n",
    "plt.plot(agg, c='blue')\n",
    "election = parse(\"2014-05-01\")\n",
    "plt.axvline(x=election, c='red')\n",
    "plt.title(\"Monthly Parliamentary Questions\", size=22)\n",
    "plt.xlabel(\"Time\", size = 18)\n",
    "plt.ylabel(\"Quantity\", size = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process headlines dataset, aggregating by month and producing datetime plot\n",
    "headline_dates = pd.DataFrame(pd.to_datetime(headlines_raw[\"publish_date\"]))\n",
    "final_headline_dates = []\n",
    "for date in list(headlines_raw[\"publish_date\"]):\n",
    "    datetime = datetime.strptime(str(date), \"%Y%m%d\")\n",
    "    final_headline_dates.append(datetime)\n",
    "final_headline_dates = pd.DataFrame(final_headline_dates)\n",
    "#plot\n",
    "plt.figure(figsize=(12,6))\n",
    "agg = final_headline_dates.resample('M', on=0).count()\n",
    "plt.plot(agg, c='blue')\n",
    "election = parse(\"2014-05-01\")\n",
    "plt.axvline(x=election, c='red')\n",
    "plt.title(\"Monthly Headlines Published\", size=22)\n",
    "plt.xlabel(\"Time\", size = 18)\n",
    "plt.ylabel(\"Quantity\", size = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Religion Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling - Religious Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TfidfVectorizer to transform religious headlines\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer_pre = TfidfVectorizer(min_df=100, stop_words=\"english\", max_df=0.8)\n",
    "rel_headlines_pre_fit = vectorizer_pre.fit(processed_religious_headlines_pre)\n",
    "X_rel_headlines_pre = vectorizer.fit_transform(processed_religious_headlines_pre).toarray()\n",
    "\n",
    "vectorizer_post = TfidfVectorizer(min_df=100, stop_words=\"english\", max_df=0.8)\n",
    "rel_headlines_post_fit = vectorizer_post.fit(processed_religious_headlines_post)\n",
    "X_rel_headlines_post = vectorizer_post.fit_transform(processed_religious_headlines_post).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate seperate topic models for pre and post\n",
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_rel_headlines_pre = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda_rel_headlines_pre.fit(X_rel_headlines_pre)\n",
    "\n",
    "lda_rel_headlines_post = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda_rel_headlines_post.fit(X_rel_headlines_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display top words for topics for pre- and post-election topics\n",
    "words = list(rel_headlines_pre_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_rel_headlines_pre.components_])\n",
    "num_top_words = 10\n",
    "\n",
    "print('Displaying the top 10 words per topic and their probabilities within the topic for pre-election relgious headlines')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()\n",
    "\n",
    "words = list(rel_headlines_post_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_rel_headlines_post.components_])\n",
    "num_top_words = 10\n",
    "\n",
    "print('Displaying the top 10 words per topic and their probabilities within the topic for post-election relgious headlines')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling - Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TfidfVectorizer to transform headlines\n",
    "##Memory intensive##\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "headline_vectorizer = TfidfVectorizer(min_df=200, stop_words=\"english\", max_df=0.8)\n",
    "headlines_fit = headline_vectorizer.fit(processed_headlines)\n",
    "X_headlines = headline_vectorizer.fit_transform(processed_headlines).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 10 topics for headlines\n",
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_headlines = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda_headlines.fit(X_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View top 20 words for each topic in order to characterize\n",
    "words = list(headlines_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_headlines.components_])\n",
    "num_top_words = 20\n",
    "\n",
    "print('Displaying the top 20 words per topic and their probabilities within the topic...')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Headlines Topics to Parliament Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove words in parliament data that don't exist within headlines data\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'tagger'])\n",
    "nlp.max_length = 10000000\n",
    "processed_answers_for_headline_topics = []\n",
    "out_of_topics_mask = []\n",
    "in_topics_mask = []\n",
    "for answer in np.array(parliament_qs[\"answer\"]):\n",
    "    parsed = nlp(str(answer))\n",
    "    processed_answer_list = []\n",
    "    out_of_topics_count = 0\n",
    "    in_topics_count = 0\n",
    "    for token in parsed:\n",
    "        if re.match('[a-zA-Z]+$', token.orth_):\n",
    "            token_lemma = token.lemma_.lower()\n",
    "            if token_lemma in headlines_fit.vocabulary_:\n",
    "                processed_answer_list.append(token_lemma)\n",
    "                in_topics_count += 1\n",
    "            else:\n",
    "                out_of_topics_count += 1\n",
    "    processed_answer_str = \" \".join(processed_answer_list)\n",
    "    processed_answers_for_headline_topics.append(processed_answer_str)\n",
    "    out_of_topics_mask.append(out_of_topics_count)\n",
    "    in_topics_mask.append(in_topics_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the parliamentary answers using the vectorizer used for the headlines\n",
    "X_answers_for_headline_topics = headline_vectorizer.transform(processed_answers_for_headline_topics[:10000]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate topic distributions for the headlines to the parliamentary answers\n",
    "answers_distribution_of_headline_topics = lda_headlines.transform(X_answers_for_headline_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum amount explained per topic, normalize, and plot\n",
    "sum_explained_per_topic = []\n",
    "for i in range(num_topics):\n",
    "    sum_explained_per_topic.append(sum(answers_distribution_of_headline_topics[:,i]))\n",
    "per_explained_per_topic = []\n",
    "for i in range(num_topics):\n",
    "    x = sum_explained_per_topic[i]/sum(sum_explained_per_topic)\n",
    "    per_explained_per_topic.append(x)\n",
    "#plot\n",
    "plt.bar(range(10), per_explained_per_topic, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Percent Explained')\n",
    "labs = ('Growth', 'Crisis', \"Local Gov't\", 'Investment', 'Crime', 'Development', 'Financial', '', 'Infrastructure', 'Politics')\n",
    "plt.xticks(np.arange(10), labs, color='orange', rotation=60, fontweight='bold', fontsize='17', horizontalalignment='right')\n",
    "plt.title('Amount of Parliamentary Answers Explained by Each Headline Topic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the amount of words unique to each dataset and shared among the datasets\n",
    "in_both = 0\n",
    "in_answers_only = 0\n",
    "in_headlines_only = 0\n",
    "for word in answers_fit.vocabulary_:\n",
    "    if word in headlines_fit.vocabulary_:\n",
    "        in_both += 1\n",
    "    else:\n",
    "        in_answers_only += 1\n",
    "for word in headlines_fit.vocabulary_:\n",
    "    if word not in answers_fit.vocabulary_:\n",
    "        in_headlines_only += 1\n",
    "print(in_answers_only, in_both, in_headlines_only)\n",
    "venn2(subsets = (in_answers_only, in_headlines_only, in_both), set_labels = (\"Unique Words in Answers\", \"Unique Words in Headlines\"),\n",
    "      set_colors=('purple', 'skyblue'), alpha = 0.7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot topic distribution for the headlines dataset itself to establish baseline\n",
    "X_headlines_sample = X_headlines[np.random.choice(X_headlines.shape[0], 10000, replace=False)]\n",
    "headline_topic_distribution = lda_headlines.transform(X_headlines_sample)\n",
    "\n",
    "sum_explained_per_topic = []\n",
    "for i in range(num_topics):\n",
    "    sum_explained_per_topic.append(sum(headline_topic_distribution[:,i]))\n",
    "per_explained_per_topic = []\n",
    "for i in range(num_topics):\n",
    "    x = sum_explained_per_topic[i]/sum(sum_explained_per_topic)\n",
    "    per_explained_per_topic.append(x)\n",
    "\n",
    "plt.bar(range(10), per_explained_per_topic, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Percent Explained')\n",
    "plt.title('Headline Topics')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
