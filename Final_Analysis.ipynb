{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob must be installed to run this code. Raw datafiles used for this analysis can be found at:\n",
    "* https://www.kaggle.com/therohk/india-headlines-news-dataset\n",
    "* https://www.kaggle.com/rajanand/rajyasabha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d566eec80060>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBlobber\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import codecs\n",
    "import spacy\n",
    "from textblob import TextBlob, Word, Blobber\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "from matplotlib_venn import venn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code borrowed and adapted from George Chen, Carnegie Mellon University#\n",
    "#Define function to remove punctuation and whitespace, and lowercase all text\n",
    "def makeWordList(str_object):\n",
    "    \n",
    "    corpus_text = str(str_object)\n",
    "    \n",
    "    for c in string.punctuation:\n",
    "        corpus_text = corpus_text.replace(c, \"\")  # -- (1)\n",
    "    \n",
    "    text = re.sub(r'\\S*\\d\\S*','',corpus_text) # -- (2)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)         # -- (3)\n",
    "    \n",
    "    text = text.lower().split()           # -- (4)         \n",
    "    \n",
    "    li = []\n",
    "    for token in text:\n",
    "        li.append(token)\n",
    "\n",
    "    return \" \".join(li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Religion Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in headlines dataset\n",
    "data = pd.read_csv(\"india-news-headlines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset for upload to GitHub\n",
    "india_news_headlines_1 = data.iloc[0:593984, :]\n",
    "india_news_headlines_2 = data.iloc[593984:1187969, :]\n",
    "india_news_headlines_3 = data.iloc[1187969:1781953, :]\n",
    "india_news_headlines_4 = data.iloc[1781953:2375938, :]\n",
    "india_news_headlines_5 = data.iloc[2375938:2969922, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write split data to csv files for upload\n",
    "india_news_headlines_1.to_csv(\"india-news-headlines-1.csv\", index=False)\n",
    "india_news_headlines_2.to_csv(\"india-news-headlines-2.csv\", index=False)\n",
    "india_news_headlines_3.to_csv(\"india-news-headlines-3.csv\", index=False)\n",
    "india_news_headlines_4.to_csv(\"india-news-headlines-4.csv\", index=False)\n",
    "india_news_headlines_5.to_csv(\"india-news-headlines-5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define religious vocabulary for creating religion-related datasets\n",
    "religious_vocab = ['religion', 'religious', 'hindu', 'hinduism',\n",
    "                  'islam', 'muslim', 'christianity', 'christian', 'sikh',\n",
    "                  'sikhism', 'temple', 'mosque', 'church', 'divine', 'god', 'gods',\n",
    "                  'prayer', 'prayers', 'priest', 'clergy', 'imam', 'monk', 'dharma',\n",
    "                  'vedas', 'worship', 'worshippers', 'worshipers' 'worshipper', 'worshiper', 'ayodhya',\n",
    "                   'babri', 'hindutva','lynching','ethnic', 'purity','nationalism', 'nationalist',\n",
    "                   'rss', 'sangh'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split headline data on 2014 election into pre and post sets\n",
    "data_pre = data[data['publish_date'] <= 20140501]\n",
    "data_post = data[data[\"publish_date\"] > 20140501]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate religious flags for pre- and post-election datasets\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'tagger'])\n",
    "nlp.max_length = 10000000\n",
    "pre_mask = []\n",
    "for headline in np.array(data_pre[\"headline_text\"]):\n",
    "    parsed = nlp(headline)\n",
    "    religious_flag = False\n",
    "    for token in parsed:\n",
    "        if re.match('[a-zA-Z]+$', token.orth_):\n",
    "            token_lemma = token.lemma_.lower()\n",
    "            if token_lemma in religious_vocab:\n",
    "                religious_flag = True\n",
    "    pre_mask.append(religious_flag)\n",
    "        \n",
    "post_mask = []\n",
    "for headline in np.array(data_post[\"headline_text\"]):\n",
    "    parsed = nlp(headline)\n",
    "    religious_flag = False\n",
    "    for token in parsed:\n",
    "        if re.match('[a-zA-Z]+$', token.orth_):\n",
    "            token_lemma = token.lemma_.lower()\n",
    "            if token_lemma in religious_vocab:\n",
    "                religious_flag = True\n",
    "    post_mask.append(religious_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate religious datasets\n",
    "pre = pd.Series(pre_mask)\n",
    "religious_pre = data_pre[pre.values]\n",
    "post = pd.Series(post_mask)\n",
    "religious_post = data_post[post.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write religious datasets to csv files\n",
    "religious_pre.to_csv(\"religious_headlines_pre.csv\", index=False)\n",
    "religious_post.to_csv(\"religious_headlines_post.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import parliamentary Q&A dataset\n",
    "li = []\n",
    "\n",
    "for file in glob.glob('./india_analysis_UDA/Parliament_Qs/rajyasabha_questions_and_answers_*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "parliament_qs = pd.concat(li, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply religion filter to parliamentary dataset\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'tagger'])\n",
    "nlp.max_length = 10000000\n",
    "answer_mask = []\n",
    "for answer in np.array(parliament_qs[\"answer\"]):\n",
    "    parsed = nlp(str(answer))\n",
    "    religious_flag = False\n",
    "    for token in parsed:\n",
    "        if re.match('[a-zA-Z]+$', token.orth_):\n",
    "            token_lemma = token.lemma_.lower()\n",
    "            if token_lemma in religious_vocab:\n",
    "                religious_flag = True\n",
    "    answer_mask.append(religious_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate religious parliamentary answer dataset\n",
    "answer_mask = pd.Series(answer_mask)\n",
    "religious_answers = parliament_qs[answer_mask.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split religious parliamentary dataset based on when the new government came into office\n",
    "answers_pre_mask = []\n",
    "for date in religious_answers['answer_date'].to_numpy():\n",
    "    if parse(date) < parse('2014.05.26'):\n",
    "        answers_pre_mask.append(True)\n",
    "    else:\n",
    "        answers_pre_mask.append(False)\n",
    "answers_pre_mask = pd.Series(answers_pre_mask)\n",
    "religious_answers_pre = religious_answers[answers_pre_mask.values]\n",
    "religious_answers_post = religious_answers[~answers_pre_mask.values]\n",
    "religious_answers_pre.to_csv(\"religious_answers_pre.csv\", index=False)\n",
    "religious_answers_post.to_csv(\"religious_answers_post.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Datasets Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create single parliament_qs dataframe with all question data\n",
    "li = []\n",
    "\n",
    "for file in glob.glob('./Parliament_Qs/rajyasabha_questions_and_answers_*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "parliament_qs = pd.concat(li, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process parliamentary answers\n",
    "processed_answers = []\n",
    "\n",
    "for str_object in list(parliament_qs[\"answer\"]):\n",
    "    processed_answers.append(makeWordList(str_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import headlines dataset\n",
    "li = []\n",
    "\n",
    "for file in glob.glob('./india_headlines_data/*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "headlines_raw = pd.concat(li, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process headlines, delete headlines object, sample 10% of processed headlines\n",
    "import random\n",
    "processed_headlines = []\n",
    "random.seed(42)\n",
    "headlines = random.sample(list(headlines_raw[\"headline_text\"]), round(len(headlines_raw)/10))\n",
    "                          \n",
    "for str_object in headlines:\n",
    "    processed_headlines.append(makeWordList(str_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import religious headline pre- and post-election datasets\n",
    "li = []\n",
    "\n",
    "for file in glob.glob('./india_headlines_data_pre/religious*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "religious_headlines_pre = pd.concat(li, axis = 0, ignore_index = True)\n",
    "\n",
    "li = []\n",
    "\n",
    "for file in glob.glob('./india_headlines_data_post/religious*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "religious_headlines_post = pd.concat(li, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process religious headlines\n",
    "processed_religious_headlines_pre = []\n",
    "\n",
    "for str_object in list(religious_headlines_pre[\"headline_text\"]):\n",
    "    processed_religious_headlines_pre.append(makeWordList(str_object))\n",
    "    \n",
    "processed_religious_headlines_post = []\n",
    "\n",
    "for str_object in list(religious_headlines_post[\"headline_text\"]):\n",
    "    processed_religious_headlines_post.append(makeWordList(str_object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Description and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Parliament Q&A dataset, aggregating by month and producing a plot of the date distribution\n",
    "answer_dates = pd.DataFrame(pd.to_datetime(parliament_qs[\"answer_date\"]))\n",
    "final_answer_dates = []\n",
    "for date in answer_dates[\"answer_date\"]:\n",
    "    if isinstance(date, datetime):\n",
    "        final_answer_dates.append([date, 1])\n",
    "final_answer_dates = pd.DataFrame(final_answer_dates)\n",
    "#final_answer_dates.reset_index().set_index(0)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "agg = final_answer_dates.resample('M', on=0).count()\n",
    "plt.plot(agg, c='blue')\n",
    "election = parse(\"2014-05-01\")\n",
    "plt.axvline(x=election, c='red')\n",
    "plt.title(\"Monthly Parliamentary Questions\", size=22)\n",
    "plt.xlabel(\"Time\", size = 18)\n",
    "plt.ylabel(\"Quantity\", size = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process headlines dataset, aggregating by month and producing datetime plot\n",
    "headline_dates = pd.DataFrame(pd.to_datetime(headlines_raw[\"publish_date\"]))\n",
    "final_headline_dates = []\n",
    "for date in list(headlines_raw[\"publish_date\"]):\n",
    "    datetime = datetime.strptime(str(date), \"%Y%m%d\")\n",
    "    final_headline_dates.append(datetime)\n",
    "final_headline_dates = pd.DataFrame(final_headline_dates)\n",
    "#plot\n",
    "plt.figure(figsize=(12,6))\n",
    "agg = final_headline_dates.resample('M', on=0).count()\n",
    "plt.plot(agg, c='blue')\n",
    "election = parse(\"2014-05-01\")\n",
    "plt.axvline(x=election, c='red')\n",
    "plt.title(\"Monthly Headlines Published\", size=22)\n",
    "plt.xlabel(\"Time\", size = 18)\n",
    "plt.ylabel(\"Quantity\", size = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Religion Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_polarity(text):\n",
    "    '''\n",
    "    Applies sentiment analysis to a piece of text.\n",
    "    Returns polarity with 1 as pos and 0 as neg.\n",
    "    '''\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pre-election headlines data\n",
    "headlines_pre = pd.read_csv('religious_headlines_pre.csv')\n",
    "headlines_pre = pd.DataFrame(headlines_pre)\n",
    "\n",
    "# Read in post-election headlines data\n",
    "headlines_post = pd.read_csv('religious_headlines_post.csv')\n",
    "headlines_post = pd.DataFrame(headlines_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find polarity of each headline in pre-election dataset\n",
    "polarity_pre = []\n",
    "for i in range(headlines_pre.shape[0]):\n",
    "    polarity_pre.append(detect_polarity(headlines_pre.iloc[i,2]))\n",
    "\n",
    "# Find polarity of each headline in post-election dataset\n",
    "polarity_post = []\n",
    "for i in range(headlines_post.shape[0]):\n",
    "    polarity_post.append(detect_polarity(headlines_post.iloc[i,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polarity scores to dataframe\n",
    "headlines_pre['Polarity'] = polarity_pre\n",
    "headlines_post['Polarity'] = polarity_post\n",
    "\n",
    "print('Pre-Election Average Sentiment:', headlines_pre['Polarity'].mean())\n",
    "print('Post-Election Average Sentiment:', headlines_post['Polarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_pre['Election'] = \"Pre\"\n",
    "headlines_post['Election'] = \"Post\"\n",
    "\n",
    "# Combine pre and post headlines into one dataframe\n",
    "all_headlines = pd.concat([headlines_pre, headlines_post])\n",
    "sns.violinplot(y='Polarity', x='Election', data=all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 50\n",
    "n, bins, patches = plt.hist(headlines_pre.Polarity, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Pre-Election: Histogram of polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 50\n",
    "n, bins, patches = plt.hist(headlines_post.Polarity, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Post-Election: Histogram of polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_pre = headlines_pre[headlines_pre['Polarity']!=0] #remove 0s to get a clearer picture\n",
    "\n",
    "num_bins = 25\n",
    "n, bins, patches = plt.hist(filter_pre.Polarity, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Pre-Election Polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter_post = headlines_post[headlines_post['Polarity']!=0] #remove 0s to get a clearer picture\n",
    "\n",
    "num_bins = 25\n",
    "n, bins, patches = plt.hist(filter_post.Polarity, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Post-Election Polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parliament Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pre-election data\n",
    "answers_pre = pd.read_csv('religious_answers_pre.csv')\n",
    "answers_pre = pd.DataFrame(answers_pre)\n",
    "\n",
    "# Read in post-election data\n",
    "answers_post = pd.read_csv('religious_answers_post.csv')\n",
    "answers_post = pd.DataFrame(answers_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find polarity of each headline in pre-election dataset\n",
    "polarity_pre = []\n",
    "for i in range(answers_pre.shape[0]):\n",
    "    polarity_pre.append(detect_polarity(answers_pre.iloc[i,2]))\n",
    "    \n",
    "# Find polarity of each headline in post-election dataset\n",
    "polarity_post = []\n",
    "for i in range(answers_post.shape[0]):\n",
    "    polarity_post.append(detect_polarity(answers_post.iloc[i,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polarity scores to dataframe\n",
    "answers_pre['Polarity'] = polarity_pre\n",
    "answers_post['Polarity'] = polarity_post\n",
    "\n",
    "print('Pre-Election Average Sentiment:', answers_pre['Polarity'].mean())\n",
    "print('Post-Election Average Sentiment:', answers_post['Polarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_pre['Election'] = \"Pre\"\n",
    "answers_post['Election'] = \"Post\"\n",
    "\n",
    "# Combine pre and post headlines into one dataframe\n",
    "all_answers = pd.concat([answers_pre, answers_post])\n",
    "sns.violinplot(y='Polarity', x='Election', data=all_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 50\n",
    "n, bins, patches = plt.hist(answers_pre.Polarity, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Pre-Election: Histogram of polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_bins = 50\n",
    "n, bins, patches = plt.hist(answers_post.Polarity, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Post-Election: Histogram of polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_pre = answers_pre[answers_pre['Polarity']!=0] #remove 0s to get a clearer picture\n",
    "\n",
    "num_bins = 50\n",
    "n, bins, patches = plt.hist(filter_pre.Polarity, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Pre-Election Polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_post = answers_post[answers_post['Polarity']!=0] #remove 0s to get a clearer picture\n",
    "\n",
    "num_bins = 50\n",
    "n, bins, patches = plt.hist(filter_post.Polarity, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Post-Election Polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling - Religious Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TfidfVectorizer to transform religious headlines\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer_pre = TfidfVectorizer(min_df=100, stop_words=\"english\", max_df=0.8)\n",
    "rel_headlines_pre_fit = vectorizer_pre.fit(processed_religious_headlines_pre)\n",
    "X_rel_headlines_pre = vectorizer_pre.fit_transform(processed_religious_headlines_pre).toarray()\n",
    "\n",
    "vectorizer_post = TfidfVectorizer(min_df=100, stop_words=\"english\", max_df=0.8)\n",
    "rel_headlines_post_fit = vectorizer_post.fit(processed_religious_headlines_post)\n",
    "X_rel_headlines_post = vectorizer_post.fit_transform(processed_religious_headlines_post).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate seperate topic models for pre and post\n",
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_rel_headlines_pre = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda_rel_headlines_pre.fit(X_rel_headlines_pre)\n",
    "\n",
    "lda_rel_headlines_post = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda_rel_headlines_post.fit(X_rel_headlines_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display top words for topics for pre- and post-election topics\n",
    "words = list(rel_headlines_pre_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_rel_headlines_pre.components_])\n",
    "num_top_words = 10\n",
    "\n",
    "print('Displaying the top 10 words per topic and their probabilities within the topic for pre-election relgious headlines')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()\n",
    "\n",
    "words = list(rel_headlines_post_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_rel_headlines_post.components_])\n",
    "num_top_words = 10\n",
    "\n",
    "print('Displaying the top 10 words per topic and their probabilities within the topic for post-election relgious headlines')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling - Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TfidfVectorizer to transform headlines\n",
    "##Memory intensive##\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "headline_vectorizer = TfidfVectorizer(min_df=200, stop_words=\"english\", max_df=0.8)\n",
    "headlines_fit = headline_vectorizer.fit(processed_headlines)\n",
    "X_headlines = headline_vectorizer.fit_transform(processed_headlines).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 10 topics for headlines\n",
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_headlines = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda_headlines.fit(X_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View top 20 words for each topic in order to characterize\n",
    "words = list(headlines_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_headlines.components_])\n",
    "num_top_words = 20\n",
    "\n",
    "print('Displaying the top 20 words per topic and their probabilities within the topic...')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Headlines Topics to Parliament Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove words in parliament data that don't exist within headlines data\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'tagger'])\n",
    "nlp.max_length = 10000000\n",
    "processed_answers_for_headline_topics = []\n",
    "out_of_topics_mask = []\n",
    "in_topics_mask = []\n",
    "for answer in np.array(parliament_qs[\"answer\"]):\n",
    "    parsed = nlp(str(answer))\n",
    "    processed_answer_list = []\n",
    "    out_of_topics_count = 0\n",
    "    in_topics_count = 0\n",
    "    for token in parsed:\n",
    "        if re.match('[a-zA-Z]+$', token.orth_):\n",
    "            token_lemma = token.lemma_.lower()\n",
    "            if token_lemma in headlines_fit.vocabulary_:\n",
    "                processed_answer_list.append(token_lemma)\n",
    "                in_topics_count += 1\n",
    "            else:\n",
    "                out_of_topics_count += 1\n",
    "    processed_answer_str = \" \".join(processed_answer_list)\n",
    "    processed_answers_for_headline_topics.append(processed_answer_str)\n",
    "    out_of_topics_mask.append(out_of_topics_count)\n",
    "    in_topics_mask.append(in_topics_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the parliamentary answers using the vectorizer used for the headlines\n",
    "X_answers_for_headline_topics = headline_vectorizer.transform(processed_answers_for_headline_topics[:10000]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate topic distributions for the headlines to the parliamentary answers\n",
    "answers_distribution_of_headline_topics = lda_headlines.transform(X_answers_for_headline_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum amount explained per topic, normalize, and plot\n",
    "sum_explained_per_topic = []\n",
    "for i in range(num_topics):\n",
    "    sum_explained_per_topic.append(sum(answers_distribution_of_headline_topics[:,i]))\n",
    "per_explained_per_topic = []\n",
    "for i in range(num_topics):\n",
    "    x = sum_explained_per_topic[i]/sum(sum_explained_per_topic)\n",
    "    per_explained_per_topic.append(x)\n",
    "#plot\n",
    "plt.bar(range(10), per_explained_per_topic, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Percent Explained')\n",
    "labs = ('Growth', 'Crisis', \"Local Gov't\", 'Investment', 'Crime', 'Development', 'Financial', '', 'Infrastructure', 'Politics')\n",
    "plt.xticks(np.arange(10), labs, color='orange', rotation=60, fontweight='bold', fontsize='17', horizontalalignment='right')\n",
    "plt.title('Amount of Parliamentary Answers Explained by Each Headline Topic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the amount of words unique to each dataset and shared among the datasets\n",
    "in_both = 0\n",
    "in_answers_only = 0\n",
    "in_headlines_only = 0\n",
    "\n",
    "#Use TfidfVectorizer to transform parliamentary answers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=200, stop_words=\"english\", max_df=0.8)\n",
    "answers_fit = vectorizer.fit(processed_answers)\n",
    "X_answers = vectorizer.fit_transform(processed_answers).toarray()\n",
    "\n",
    "for word in answers_fit.vocabulary_:\n",
    "    if word in headlines_fit.vocabulary_:\n",
    "        in_both += 1\n",
    "    else:\n",
    "        in_answers_only += 1\n",
    "for word in headlines_fit.vocabulary_:\n",
    "    if word not in answers_fit.vocabulary_:\n",
    "        in_headlines_only += 1\n",
    "print(in_answers_only, in_both, in_headlines_only)\n",
    "venn2(subsets = (in_answers_only, in_headlines_only, in_both), set_labels = (\"Unique Words in Answers\", \"Unique Words in Headlines\"),\n",
    "      set_colors=('purple', 'skyblue'), alpha = 0.7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot topic distribution for the headlines dataset itself to establish baseline\n",
    "X_headlines_sample = X_headlines[np.random.choice(X_headlines.shape[0], 10000, replace=False)]\n",
    "headline_topic_distribution = lda_headlines.transform(X_headlines_sample)\n",
    "\n",
    "sum_explained_per_topic = []\n",
    "for i in range(num_topics):\n",
    "    sum_explained_per_topic.append(sum(headline_topic_distribution[:,i]))\n",
    "per_explained_per_topic = []\n",
    "for i in range(num_topics):\n",
    "    x = sum_explained_per_topic[i]/sum(sum_explained_per_topic)\n",
    "    per_explained_per_topic.append(x)\n",
    "\n",
    "plt.bar(range(10), per_explained_per_topic, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Percent Explained')\n",
    "plt.title('Headline Topics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
