{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deron\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Create single parliament_qs dataframe with all question data\n",
    "li = []\n",
    "\n",
    "for file in glob.glob('./Parliament_Qs/rajyasabha_questions_and_answers_*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "parliament_qs = pd.concat(li, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code borrowed and adapted from George Chen, Carnegie Mellon University#\n",
    "#Define function to remove punctuation and whitespace, and lowercase all text\n",
    "def makeWordList(str_object):\n",
    "    \n",
    "    corpus_text = str(str_object)\n",
    "    \n",
    "    for c in string.punctuation:\n",
    "        corpus_text = corpus_text.replace(c, \"\")  # -- (1)\n",
    "    \n",
    "    text = re.sub(r'\\S*\\d\\S*','',corpus_text) # -- (2)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)         # -- (3)\n",
    "    \n",
    "    text = text.lower().split()           # -- (4)         \n",
    "    \n",
    "    li = []\n",
    "    for token in text:\n",
    "        li.append(token)\n",
    "\n",
    "    return \" \".join(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the questions\n",
    "processed_questions = []\n",
    "\n",
    "for str_object in list(parliament_qs[\"question_description\"]):\n",
    "    processed_questions.append(makeWordList(str_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process responses\n",
    "processed_answers = []\n",
    "\n",
    "for str_object in list(parliament_qs[\"answer\"]):\n",
    "    processed_answers.append(makeWordList(str_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TfidfVectorizer to transform parliamentary questions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=200, stop_words=\"english\", max_df=0.8)\n",
    "questions_fit = vectorizer.fit(processed_questions)\n",
    "X_questions = vectorizer.fit_transform(processed_questions).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!Time-consuming!#\n",
    "#Create topics using LDA\n",
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda.fit(X_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 10 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "gap : 0.01579022441204997\n",
      "infrastructure : 0.014897226390985315\n",
      "prevention : 0.014573223734675381\n",
      "bureau : 0.014133207481119538\n",
      "proportion : 0.012630412616072841\n",
      "industrial : 0.012042821713099805\n",
      "hilly : 0.012024388809306048\n",
      "directed : 0.010113664162308359\n",
      "partnership : 0.00971378763978077\n",
      "energy : 0.00963057648174496\n",
      "\n",
      "[Topic 1]\n",
      "schools : 0.028377798470822658\n",
      "value : 0.022452517968475666\n",
      "mission : 0.015641742441613003\n",
      "vegetables : 0.014005590929451263\n",
      "cil : 0.012541512057791986\n",
      "infrastructure : 0.012243526742358556\n",
      "attention : 0.011411070447612725\n",
      "aviation : 0.011187747979636595\n",
      "completely : 0.011059364577094612\n",
      "directed : 0.010075847135726523\n",
      "\n",
      "[Topic 2]\n",
      "laying : 0.017437642339725355\n",
      "gap : 0.016036999598692318\n",
      "interests : 0.015574737920312092\n",
      "suffering : 0.015011090032775643\n",
      "amend : 0.014575143537023516\n",
      "granted : 0.014128134505889766\n",
      "infrastructure : 0.014042519723800414\n",
      "speed : 0.012748081485848047\n",
      "toll : 0.011226769609422156\n",
      "delhib : 0.009981605449072763\n",
      "\n",
      "[Topic 3]\n",
      "gap : 0.01665735552810064\n",
      "infrastructure : 0.01599102032680464\n",
      "proportion : 0.013350321109316447\n",
      "farmers : 0.012506910627981585\n",
      "hilly : 0.009730435685596863\n",
      "yearb : 0.008346558333719747\n",
      "directed : 0.008121052439703992\n",
      "partnership : 0.007771927861958956\n",
      "till : 0.006928126695967104\n",
      "nuclear : 0.006777506268512713\n",
      "\n",
      "[Topic 4]\n",
      "gap : 0.019707143011491592\n",
      "infrastructure : 0.01822681493073893\n",
      "partnership : 0.01468660287499536\n",
      "proportion : 0.014302206505072348\n",
      "directed : 0.013827337512302465\n",
      "crore : 0.011018530229529262\n",
      "visavis : 0.010458743743224077\n",
      "energy : 0.010196371428833501\n",
      "till : 0.009161007849768652\n",
      "laid : 0.00836136434451555\n",
      "\n",
      "[Topic 5]\n",
      "standard : 0.03854705184466069\n",
      "accidents : 0.03090269869122999\n",
      "colleges : 0.0241941550593889\n",
      "andb : 0.020061945272628813\n",
      "organisation : 0.01853957584032026\n",
      "threat : 0.018515469332550917\n",
      "forests : 0.018445427572403493\n",
      "ganga : 0.01642517680162226\n",
      "aircraft : 0.01523273566432073\n",
      "functional : 0.014190078262292785\n",
      "\n",
      "[Topic 6]\n",
      "november : 0.01697450805563663\n",
      "northeastern : 0.01633757387241586\n",
      "infrastructure : 0.016073179300577303\n",
      "gap : 0.01598557965338409\n",
      "proportion : 0.013639433405762954\n",
      "abroad : 0.012746588075197532\n",
      "neighbouring : 0.012461342378351774\n",
      "requests : 0.011809997717647451\n",
      "wing : 0.011554194096799466\n",
      "directed : 0.011375709214460442\n",
      "\n",
      "[Topic 7]\n",
      "infrastructure : 0.018176053877139543\n",
      "gap : 0.015253866153471178\n",
      "agencies : 0.013956966719513842\n",
      "regardc : 0.011393612740228294\n",
      "entered : 0.011225144691594274\n",
      "proportion : 0.010702727435117737\n",
      "anddif : 0.010415153788265437\n",
      "success : 0.01019039318447461\n",
      "threat : 0.00975922269189214\n",
      "unit : 0.009472223542003158\n",
      "\n",
      "[Topic 8]\n",
      "regular : 0.04491061928953913\n",
      "laid : 0.03922825784051258\n",
      "protect : 0.029443264204232433\n",
      "constructed : 0.02517628102751981\n",
      "framed : 0.024453057794670437\n",
      "proposing : 0.018960402772945396\n",
      "sites : 0.018409789110487414\n",
      "gram : 0.017280117518291272\n",
      "clean : 0.013805307813359261\n",
      "ready : 0.013028155486766084\n",
      "\n",
      "[Topic 9]\n",
      "committed : 0.017048730005564817\n",
      "gap : 0.015346815793750614\n",
      "yearwiseb : 0.012011097577610423\n",
      "procedure : 0.011877185205741913\n",
      "infrastructure : 0.011537496273651509\n",
      "clearances : 0.010774941793517599\n",
      "laying : 0.010684555155997368\n",
      "appointment : 0.01059811380645976\n",
      "sadak : 0.010253520553817069\n",
      "partnership : 0.010202251242088302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display top 10 words from each topic\n",
    "words = list(questions_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda.components_])\n",
    "num_top_words = 10\n",
    "\n",
    "print('Displaying the top 10 words per topic and their probabilities within the topic...')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TfidfVectorizer to transform parliamentary answers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=200, stop_words=\"english\", max_df=0.8)\n",
    "answers_fit = vectorizer.fit(processed_answers)\n",
    "X_answers = vectorizer.fit_transform(processed_answers).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!Time-consuming!#\n",
    "#Create topics using LDA\n",
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_answers = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda_answers.fit(X_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 10 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "factories : 0.02988721628112838\n",
      "obcs : 0.02096461145550225\n",
      "turn : 0.01278346755295074\n",
      "telecommunication : 0.012244657374213581\n",
      "tushar : 0.011854834372685976\n",
      "eia : 0.010469027885389176\n",
      "brgf : 0.01040769803138051\n",
      "nadu : 0.010387606802206852\n",
      "age : 0.010066275250836669\n",
      "yesso : 0.00893199274924593\n",
      "\n",
      "[Topic 1]\n",
      "modes : 0.054054773288766136\n",
      "adhere : 0.053096678905447885\n",
      "delays : 0.05243857271473336\n",
      "output : 0.050170072321687506\n",
      "pala : 0.027718365872471375\n",
      "thereunder : 0.024624474760197763\n",
      "bulk : 0.021081139431236313\n",
      "load : 0.020522358079513296\n",
      "currency : 0.020150267589209212\n",
      "deficiencies : 0.019329537512068926\n",
      "\n",
      "[Topic 2]\n",
      "audit : 0.04533189827779601\n",
      "exercise : 0.02308047410385203\n",
      "points : 0.022290914541281563\n",
      "plan : 0.019666558375040173\n",
      "measure : 0.01692254929341677\n",
      "doctors : 0.015094389664007071\n",
      "gurjara : 0.01447741111652817\n",
      "monitoring : 0.013979563620813908\n",
      "rules : 0.01294867238909764\n",
      "widening : 0.012848569270351915\n",
      "\n",
      "[Topic 3]\n",
      "janani : 0.04040458133266948\n",
      "benefitted : 0.030239554344322413\n",
      "cctv : 0.01989625979978034\n",
      "nrlm : 0.019131391493969495\n",
      "doctors : 0.018898021235902163\n",
      "brazil : 0.01883199024415634\n",
      "inclusion : 0.0184205458895673\n",
      "maneka : 0.016853669523115927\n",
      "seen : 0.014998248867211373\n",
      "loss : 0.010499107022425914\n",
      "\n",
      "[Topic 4]\n",
      "completely : 0.026200276866783917\n",
      "annexurei : 0.02517146825503894\n",
      "declaration : 0.021628139199445492\n",
      "need : 0.020904778322942963\n",
      "vincent : 0.020853241136526098\n",
      "tsp : 0.017639352225067354\n",
      "keeping : 0.016448041867277448\n",
      "view : 0.016352341351628313\n",
      "infant : 0.013538805250939324\n",
      "based : 0.01334918216931553\n",
      "\n",
      "[Topic 5]\n",
      "matters : 0.005372473692886925\n",
      "putting : 0.005053500850263453\n",
      "load : 0.005012915063619389\n",
      "combat : 0.0038102134207242556\n",
      "admission : 0.003623092181044097\n",
      "average : 0.003544287976541322\n",
      "flow : 0.0032212590668873145\n",
      "nominated : 0.0031535640145784975\n",
      "weak : 0.0031133853556696116\n",
      "minimize : 0.003031475606202433\n",
      "\n",
      "[Topic 6]\n",
      "farm : 0.010890750553972333\n",
      "governing : 0.010631591927535298\n",
      "export : 0.01042761717855176\n",
      "ahmed : 0.00966372365346281\n",
      "turn : 0.00903506889780089\n",
      "budgetary : 0.008569917721421114\n",
      "ayush : 0.007883474283675645\n",
      "sea : 0.007635379562045441\n",
      "installation : 0.0073810019951607705\n",
      "expression : 0.007260376520362829\n",
      "\n",
      "[Topic 7]\n",
      "packages : 0.010392981253693648\n",
      "particularly : 0.010153576943929302\n",
      "combat : 0.009422426277671066\n",
      "utilised : 0.006957463811331274\n",
      "instances : 0.0067098594704957095\n",
      "vegetable : 0.00544851053645141\n",
      "operation : 0.0053813576357703\n",
      "corporate : 0.005320501495911076\n",
      "rolling : 0.005134169735011386\n",
      "remain : 0.0050676703595140355\n",
      "\n",
      "[Topic 8]\n",
      "dharmendra : 0.009104573175141687\n",
      "tenders : 0.008566112531832665\n",
      "ready : 0.007321276118325705\n",
      "input : 0.007248460585433207\n",
      "highwaysshri : 0.006729270061312966\n",
      "outcome : 0.006345446779905934\n",
      "formula : 0.006303279106942731\n",
      "pointed : 0.006132038229994863\n",
      "load : 0.005881463912567714\n",
      "wing : 0.0057556947756463965\n",
      "\n",
      "[Topic 9]\n",
      "detect : 0.019574049644961084\n",
      "chhattisgarh : 0.018240421039424633\n",
      "comprises : 0.010116135506581036\n",
      "quick : 0.010082465645523902\n",
      "retail : 0.008198467051153152\n",
      "concessional : 0.00779173308663773\n",
      "formulating : 0.0070511971431038364\n",
      "organizing : 0.00700593697010523\n",
      "weeks : 0.006963759442851396\n",
      "input : 0.005963987629620438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display top 10 words from each topic\n",
    "words = list(answers_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_answers.components_])\n",
    "num_top_words = 10\n",
    "\n",
    "print('Displaying the top 10 words per topic and their probabilities within the topic...')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
