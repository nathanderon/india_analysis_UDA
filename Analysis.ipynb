{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create single parliament_qs dataframe with all question data\n",
    "li = []\n",
    "\n",
    "for file in glob.glob('./Parliament_Qs/rajyasabha_questions_and_answers_*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "parliament_qs = pd.concat(li, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code borrowed and adapted from George Chen, Carnegie Mellon University#\n",
    "#Define function to remove punctuation and whitespace, and lowercase all text\n",
    "def makeWordList(str_object):\n",
    "    \n",
    "    corpus_text = str(str_object)\n",
    "    \n",
    "    for c in string.punctuation:\n",
    "        corpus_text = corpus_text.replace(c, \"\")  # -- (1)\n",
    "    \n",
    "    text = re.sub(r'\\S*\\d\\S*','',corpus_text) # -- (2)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)         # -- (3)\n",
    "    \n",
    "    text = text.lower().split()           # -- (4)         \n",
    "    \n",
    "    li = []\n",
    "    for token in text:\n",
    "        li.append(token)\n",
    "\n",
    "    return \" \".join(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the questions\n",
    "processed_questions = []\n",
    "\n",
    "for str_object in list(parliament_qs[\"question_description\"]):\n",
    "    processed_questions.append(makeWordList(str_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process responses\n",
    "processed_answers = []\n",
    "\n",
    "for str_object in list(parliament_qs[\"answer\"]):\n",
    "    processed_answers.append(makeWordList(str_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TfidfVectorizer to transform parliamentary questions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=200, stop_words=\"english\", max_df=0.8)\n",
    "questions_fit = vectorizer.fit(processed_questions)\n",
    "X_questions = vectorizer.fit_transform(processed_questions).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!Time-consuming!#\n",
    "#Create topics using LDA\n",
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda.fit(X_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display top 10 words from each topic\n",
    "words = list(questions_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda.components_])\n",
    "num_top_words = 10\n",
    "\n",
    "print('Displaying the top 10 words per topic and their probabilities within the topic...')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TfidfVectorizer to transform parliamentary answers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=200, stop_words=\"english\", max_df=0.8)\n",
    "answers_fit = vectorizer.fit(processed_answers)\n",
    "X_answers = vectorizer.fit_transform(processed_answers).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!Time-consuming!#\n",
    "#Create topics using LDA\n",
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_answers = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda_answers.fit(X_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display top 10 words from each topic\n",
    "words = list(answers_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_answers.components_])\n",
    "num_top_words = 10\n",
    "\n",
    "print('Displaying the top 10 words per topic and their probabilities within the topic...')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_answers.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print religious word occurances\n",
    "words = list(answers_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_answers.components_])\n",
    "res = [words.index(religious_word) for religious_word in religious_vocab]\n",
    "\n",
    "#print('Displaying the top 10 words per topic and their probabilities within the topic...')\n",
    "#print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for i in res:\n",
    "        print(words[i], ':', topic_word_distributions[topic_idx, i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "\n",
    "for file in glob.glob('./india_headlines_data/*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "headlines = pd.concat(li, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process headlines, delete headlines object, sample 10% of processed headlines\n",
    "import random\n",
    "processed_headlines = []\n",
    "random.seed(42)\n",
    "headlines = random.sample(list(headlines[\"headline_text\"]), round(len(headlines)/10))\n",
    "                          \n",
    "for str_object in headlines:\n",
    "    processed_headlines.append(makeWordList(str_object))\n",
    "del headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TfidfVectorizer to transform headlines\n",
    "##Memory intensive##\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=200, stop_words=\"english\", max_df=0.8)\n",
    "headlines_fit = vectorizer.fit(processed_headlines)\n",
    "X_headlines = vectorizer.fit_transform(processed_headlines).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_headlines = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda_headlines.fit(X_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 10 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "labour : 0.03860330906379886\n",
      "mission : 0.03374031671009788\n",
      "course : 0.032462592063320285\n",
      "cheating : 0.02674135554586314\n",
      "filed : 0.02570318762380012\n",
      "education : 0.01826181535798846\n",
      "cop : 0.017738767759230903\n",
      "scare : 0.014018263958041628\n",
      "chhattisgarh : 0.013574345666579408\n",
      "bar : 0.013280687501206163\n",
      "\n",
      "[Topic 1]\n",
      "bollywood : 0.05652672305509811\n",
      "mc : 0.020877611782112865\n",
      "chinese : 0.01951978015539641\n",
      "cell : 0.01862620140313285\n",
      "fuel : 0.018525507570669772\n",
      "common : 0.0182746225720331\n",
      "fears : 0.018235411934387834\n",
      "caught : 0.018131542869644003\n",
      "kochi : 0.017012086919733094\n",
      "trouble : 0.016547194568394756\n",
      "\n",
      "[Topic 2]\n",
      "jharkhand : 0.055251359928895634\n",
      "years : 0.03599075911791327\n",
      "nepal : 0.033688461281122864\n",
      "extended : 0.02997444049795326\n",
      "fun : 0.022083075481717694\n",
      "improve : 0.021661434481104202\n",
      "small : 0.02118834296957338\n",
      "long : 0.02072460553727788\n",
      "asks : 0.020401371771194214\n",
      "boycott : 0.0155555559639116\n",
      "\n",
      "[Topic 3]\n",
      "buildings : 0.06800078517272543\n",
      "scheme : 0.04917474487815168\n",
      "rss : 0.03973805696461748\n",
      "look : 0.02895268937944588\n",
      "ali : 0.01712305113097134\n",
      "administration : 0.0166104450346476\n",
      "singh : 0.01659478801264431\n",
      "industrial : 0.01621522219753681\n",
      "jail : 0.015302214287017762\n",
      "tomorrow : 0.014688612860199625\n",
      "\n",
      "[Topic 4]\n",
      "release : 0.04360621294294105\n",
      "inside : 0.03414299527793981\n",
      "attacked : 0.03150263701557244\n",
      "harassment : 0.02339837338809184\n",
      "problems : 0.021948066835225377\n",
      "steel : 0.016536643931746146\n",
      "photos : 0.01485718437622983\n",
      "supreme : 0.014355986192179827\n",
      "theatre : 0.013181693301407237\n",
      "jaipur : 0.013139056282036536\n",
      "\n",
      "[Topic 5]\n",
      "turn : 0.03433462592442295\n",
      "higher : 0.032536272046133685\n",
      "week : 0.03205882675072191\n",
      "companies : 0.02940898070205907\n",
      "fall : 0.02206351963952968\n",
      "things : 0.0200809237732601\n",
      "boy : 0.019838269378052206\n",
      "builders : 0.019764934195305596\n",
      "grand : 0.018013831062081192\n",
      "ground : 0.017377184996352407\n",
      "\n",
      "[Topic 6]\n",
      "cool : 0.028517561734664167\n",
      "quits : 0.027221875217268366\n",
      "getting : 0.02635653446884795\n",
      "phone : 0.02315175729112852\n",
      "rbi : 0.022298864071834915\n",
      "parliament : 0.02010383047046077\n",
      "busted : 0.017550960242662835\n",
      "solar : 0.01725186238443246\n",
      "save : 0.016498580100627224\n",
      "escape : 0.015396452105634605\n",
      "\n",
      "[Topic 7]\n",
      "telangana : 0.03314667263013821\n",
      "award : 0.029980084304023547\n",
      "hunt : 0.029255679785209928\n",
      "exam : 0.027718525390860952\n",
      "roll : 0.02395691651396087\n",
      "trying : 0.021926253271452777\n",
      "names : 0.021438804122905147\n",
      "oil : 0.021234590894291784\n",
      "debut : 0.019364755485469908\n",
      "nri : 0.018694471419408477\n",
      "\n",
      "[Topic 8]\n",
      "official : 0.026551332935362503\n",
      "lift : 0.02241211085917245\n",
      "space : 0.020653707982723434\n",
      "launched : 0.019807879454650856\n",
      "modi : 0.019145338241935306\n",
      "crore : 0.018389984229272537\n",
      "assam : 0.016761503630385748\n",
      "prime : 0.015582072377491471\n",
      "pm : 0.01543368222449356\n",
      "killing : 0.013800255673658432\n",
      "\n",
      "[Topic 9]\n",
      "courts : 0.04133147480119388\n",
      "diesel : 0.02970682249571218\n",
      "marks : 0.02454509748669973\n",
      "ministers : 0.01651001975368271\n",
      "prices : 0.016117700973804713\n",
      "game : 0.015989491711399968\n",
      "parties : 0.01584358605929998\n",
      "president : 0.01564022864279222\n",
      "crash : 0.014558989955177227\n",
      "level : 0.014127343108277362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = list(headlines_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_headlines.components_])\n",
    "num_top_words = 10\n",
    "\n",
    "print('Displaying the top 10 words per topic and their probabilities within the topic...')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "religious_vocab = ['religion', 'religious', 'hindu', 'hinduism',\n",
    "                  'islam', 'muslim', 'christianity', 'christian', 'sikh',\n",
    "                  'sikhism', 'temple', 'mosque', 'church', 'divine', 'god', 'gods',\n",
    "                  'prayer', 'prayers', 'priest', 'clergy', 'imam', 'monk', 'dharma',\n",
    "                  'vedas', 'worship', 'worshippers', 'worshipers' 'worshipper', 'worshiper', 'ayodhya',\n",
    "                   'babri', 'hindutva','lynching','ethnic', 'purity','nationalism', 'nationalist',\n",
    "                   'RSS', 'Sangh'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Topic 0]\n",
      "hindu : 2.2126267961109092e-06\n",
      "muslim : 2.2126632504544267e-06\n",
      "sikh : 2.212680231943285e-06\n",
      "temple : 0.0035887591966975636\n",
      "ayodhya : 2.2126736874920206e-06\n",
      "\n",
      "[Topic 1]\n",
      "hindu : 2.2161415766330643e-06\n",
      "muslim : 0.004624529335485363\n",
      "sikh : 2.2161928735832024e-06\n",
      "temple : 2.216194719511279e-06\n",
      "ayodhya : 2.216174563435766e-06\n",
      "\n",
      "[Topic 2]\n",
      "hindu : 2.2083110791637973e-06\n",
      "muslim : 2.208398920276732e-06\n",
      "sikh : 2.208434508756428e-06\n",
      "temple : 2.20835559935458e-06\n",
      "ayodhya : 2.2083081329043313e-06\n",
      "\n",
      "[Topic 3]\n",
      "hindu : 2.3340599834284005e-06\n",
      "muslim : 2.334167342718934e-06\n",
      "sikh : 2.3340928214179743e-06\n",
      "temple : 2.334053802147635e-06\n",
      "ayodhya : 2.334051079747267e-06\n",
      "\n",
      "[Topic 4]\n",
      "hindu : 0.005796666027080499\n",
      "muslim : 2.3100642700551904e-06\n",
      "sikh : 2.3100408635802277e-06\n",
      "temple : 2.3100521151896305e-06\n",
      "ayodhya : 0.0045679768233735825\n",
      "\n",
      "[Topic 5]\n",
      "hindu : 2.2040054997802598e-06\n",
      "muslim : 2.2040872018022866e-06\n",
      "sikh : 2.2040642514366356e-06\n",
      "temple : 2.204035097352963e-06\n",
      "ayodhya : 2.204007464315639e-06\n",
      "\n",
      "[Topic 6]\n",
      "hindu : 2.104015662381107e-06\n",
      "muslim : 2.1040366081261875e-06\n",
      "sikh : 2.1041015969599117e-06\n",
      "temple : 2.1040111944820734e-06\n",
      "ayodhya : 2.104044823145903e-06\n",
      "\n",
      "[Topic 7]\n",
      "hindu : 2.3247921124597673e-06\n",
      "muslim : 2.324893448774435e-06\n",
      "sikh : 2.3249009141828203e-06\n",
      "temple : 2.3247863456650505e-06\n",
      "ayodhya : 2.3248288847320257e-06\n",
      "\n",
      "[Topic 8]\n",
      "hindu : 2.212858814811219e-06\n",
      "muslim : 2.212927492899955e-06\n",
      "sikh : 0.009213532944994286\n",
      "temple : 2.2128751376021938e-06\n",
      "ayodhya : 2.212847995981425e-06\n",
      "\n",
      "[Topic 9]\n",
      "hindu : 1.9745113496162807e-06\n",
      "muslim : 1.974599144028853e-06\n",
      "sikh : 1.9745651335953534e-06\n",
      "temple : 1.9744906587978814e-06\n",
      "ayodhya : 1.9745185960589294e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print religious word occurances\n",
    "words = list(headlines_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_headlines.components_])\n",
    "\n",
    "res = []\n",
    "for religious_word in religious_vocab:\n",
    "    if religious_word in words:\n",
    "        res.append(words.index(religious_word))\n",
    "#res = [words.index(religious_word) for religious_word in religious_vocab]\n",
    "\n",
    "#print('Displaying the top 10 words per topic and their probabilities within the topic...')\n",
    "#print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for i in res:\n",
    "        print(words[i], ':', topic_word_distributions[topic_idx, i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
