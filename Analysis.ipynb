{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create single parliament_qs dataframe with all question data\n",
    "li = []\n",
    "\n",
    "for file in glob.glob('./Parliament_Qs/rajyasabha_questions_and_answers_*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "parliament_qs = pd.concat(li, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code borrowed and adapted from George Chen, Carnegie Mellon University#\n",
    "#Define function to remove punctuation and whitespace, and lowercase all text\n",
    "def makeWordList(str_object):\n",
    "    \n",
    "    corpus_text = str(str_object)\n",
    "    \n",
    "    for c in string.punctuation:\n",
    "        corpus_text = corpus_text.replace(c, \"\")  # -- (1)\n",
    "    \n",
    "    text = re.sub(r'\\S*\\d\\S*','',corpus_text) # -- (2)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)         # -- (3)\n",
    "    \n",
    "    text = text.lower().split()           # -- (4)         \n",
    "    \n",
    "    li = []\n",
    "    for token in text:\n",
    "        li.append(token)\n",
    "\n",
    "    return \" \".join(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the questions\n",
    "processed_questions = []\n",
    "\n",
    "for str_object in list(parliament_qs[\"question_description\"]):\n",
    "    processed_questions.append(makeWordList(str_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process responses\n",
    "processed_answers = []\n",
    "\n",
    "for str_object in list(parliament_qs[\"answer\"]):\n",
    "    processed_answers.append(makeWordList(str_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TfidfVectorizer to transform parliamentary questions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=200, stop_words=\"english\", max_df=0.8)\n",
    "questions_fit = vectorizer.fit(processed_questions)\n",
    "X_questions = vectorizer.fit_transform(processed_questions).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!Time-consuming!#\n",
    "#Create topics using LDA\n",
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda.fit(X_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display top 10 words from each topic\n",
    "words = list(questions_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda.components_])\n",
    "num_top_words = 10\n",
    "\n",
    "print('Displaying the top 10 words per topic and their probabilities within the topic...')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TfidfVectorizer to transform parliamentary answers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=200, stop_words=\"english\", max_df=0.8)\n",
    "answers_fit = vectorizer.fit(processed_answers)\n",
    "X_answers = vectorizer.fit_transform(processed_answers).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!Time-consuming!#\n",
    "#Create topics using LDA\n",
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_answers = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda_answers.fit(X_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display top 10 words from each topic\n",
    "words = list(answers_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_answers.components_])\n",
    "num_top_words = 10\n",
    "\n",
    "print('Displaying the top 10 words per topic and their probabilities within the topic...')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_answers.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print religious word occurances\n",
    "words = list(answers_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_answers.components_])\n",
    "res = [words.index(religious_word) for religious_word in religious_vocab]\n",
    "\n",
    "#print('Displaying the top 10 words per topic and their probabilities within the topic...')\n",
    "#print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for i in res:\n",
    "        print(words[i], ':', topic_word_distributions[topic_idx, i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "\n",
    "for file in glob.glob('./india_headlines_data/*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "headlines = pd.concat(li, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process headlines, delete headlines object, sample 10% of processed headlines\n",
    "import random\n",
    "processed_headlines = []\n",
    "random.seed(42)\n",
    "headlines = random.sample(list(headlines[\"headline_text\"]), round(len(headlines)/10))\n",
    "                          \n",
    "for str_object in headlines:\n",
    "    processed_headlines.append(makeWordList(str_object))\n",
    "del headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TfidfVectorizer to transform headlines\n",
    "##Memory intensive##\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=200, stop_words=\"english\", max_df=0.8)\n",
    "headlines_fit = vectorizer.fit(processed_headlines)\n",
    "X_headlines = vectorizer.fit_transform(processed_headlines).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_headlines = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda_headlines.fit(X_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 10 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "labour : 0.03860330906379886\n",
      "mission : 0.03374031671009788\n",
      "course : 0.032462592063320285\n",
      "cheating : 0.02674135554586314\n",
      "filed : 0.02570318762380012\n",
      "education : 0.01826181535798846\n",
      "cop : 0.017738767759230903\n",
      "scare : 0.014018263958041628\n",
      "chhattisgarh : 0.013574345666579408\n",
      "bar : 0.013280687501206163\n",
      "\n",
      "[Topic 1]\n",
      "bollywood : 0.05652672305509811\n",
      "mc : 0.020877611782112865\n",
      "chinese : 0.01951978015539641\n",
      "cell : 0.01862620140313285\n",
      "fuel : 0.018525507570669772\n",
      "common : 0.0182746225720331\n",
      "fears : 0.018235411934387834\n",
      "caught : 0.018131542869644003\n",
      "kochi : 0.017012086919733094\n",
      "trouble : 0.016547194568394756\n",
      "\n",
      "[Topic 2]\n",
      "jharkhand : 0.055251359928895634\n",
      "years : 0.03599075911791327\n",
      "nepal : 0.033688461281122864\n",
      "extended : 0.02997444049795326\n",
      "fun : 0.022083075481717694\n",
      "improve : 0.021661434481104202\n",
      "small : 0.02118834296957338\n",
      "long : 0.02072460553727788\n",
      "asks : 0.020401371771194214\n",
      "boycott : 0.0155555559639116\n",
      "\n",
      "[Topic 3]\n",
      "buildings : 0.06800078517272543\n",
      "scheme : 0.04917474487815168\n",
      "rss : 0.03973805696461748\n",
      "look : 0.02895268937944588\n",
      "ali : 0.01712305113097134\n",
      "administration : 0.0166104450346476\n",
      "singh : 0.01659478801264431\n",
      "industrial : 0.01621522219753681\n",
      "jail : 0.015302214287017762\n",
      "tomorrow : 0.014688612860199625\n",
      "\n",
      "[Topic 4]\n",
      "release : 0.04360621294294105\n",
      "inside : 0.03414299527793981\n",
      "attacked : 0.03150263701557244\n",
      "harassment : 0.02339837338809184\n",
      "problems : 0.021948066835225377\n",
      "steel : 0.016536643931746146\n",
      "photos : 0.01485718437622983\n",
      "supreme : 0.014355986192179827\n",
      "theatre : 0.013181693301407237\n",
      "jaipur : 0.013139056282036536\n",
      "\n",
      "[Topic 5]\n",
      "turn : 0.03433462592442295\n",
      "higher : 0.032536272046133685\n",
      "week : 0.03205882675072191\n",
      "companies : 0.02940898070205907\n",
      "fall : 0.02206351963952968\n",
      "things : 0.0200809237732601\n",
      "boy : 0.019838269378052206\n",
      "builders : 0.019764934195305596\n",
      "grand : 0.018013831062081192\n",
      "ground : 0.017377184996352407\n",
      "\n",
      "[Topic 6]\n",
      "cool : 0.028517561734664167\n",
      "quits : 0.027221875217268366\n",
      "getting : 0.02635653446884795\n",
      "phone : 0.02315175729112852\n",
      "rbi : 0.022298864071834915\n",
      "parliament : 0.02010383047046077\n",
      "busted : 0.017550960242662835\n",
      "solar : 0.01725186238443246\n",
      "save : 0.016498580100627224\n",
      "escape : 0.015396452105634605\n",
      "\n",
      "[Topic 7]\n",
      "telangana : 0.03314667263013821\n",
      "award : 0.029980084304023547\n",
      "hunt : 0.029255679785209928\n",
      "exam : 0.027718525390860952\n",
      "roll : 0.02395691651396087\n",
      "trying : 0.021926253271452777\n",
      "names : 0.021438804122905147\n",
      "oil : 0.021234590894291784\n",
      "debut : 0.019364755485469908\n",
      "nri : 0.018694471419408477\n",
      "\n",
      "[Topic 8]\n",
      "official : 0.026551332935362503\n",
      "lift : 0.02241211085917245\n",
      "space : 0.020653707982723434\n",
      "launched : 0.019807879454650856\n",
      "modi : 0.019145338241935306\n",
      "crore : 0.018389984229272537\n",
      "assam : 0.016761503630385748\n",
      "prime : 0.015582072377491471\n",
      "pm : 0.01543368222449356\n",
      "killing : 0.013800255673658432\n",
      "\n",
      "[Topic 9]\n",
      "courts : 0.04133147480119388\n",
      "diesel : 0.02970682249571218\n",
      "marks : 0.02454509748669973\n",
      "ministers : 0.01651001975368271\n",
      "prices : 0.016117700973804713\n",
      "game : 0.015989491711399968\n",
      "parties : 0.01584358605929998\n",
      "president : 0.01564022864279222\n",
      "crash : 0.014558989955177227\n",
      "level : 0.014127343108277362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = list(headlines_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_headlines.components_])\n",
    "num_top_words = 10\n",
    "\n",
    "print('Displaying the top 10 words per topic and their probabilities within the topic...')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "religious_vocab = ['religion', 'religious', 'hindu', 'hinduism',\n",
    "                  'islam', 'muslim', 'christianity', 'christian', 'sikh',\n",
    "                  'sikhism', 'temple', 'mosque', 'church', 'divine', 'god', 'gods',\n",
    "                  'prayer', 'prayers', 'priest', 'clergy', 'imam', 'monk', 'dharma',\n",
    "                  'vedas', 'worship', 'worshippers', 'worshipers' 'worshipper', 'worshiper', 'ayodhya',\n",
    "                   'babri', 'hindutva','lynching','ethnic', 'purity','nationalism', 'nationalist',\n",
    "                   'RSS', 'Sangh'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Topic 0]\n",
      "hindu : 2.2126267961109092e-06\n",
      "muslim : 2.2126632504544267e-06\n",
      "sikh : 2.212680231943285e-06\n",
      "temple : 0.0035887591966975636\n",
      "ayodhya : 2.2126736874920206e-06\n",
      "\n",
      "[Topic 1]\n",
      "hindu : 2.2161415766330643e-06\n",
      "muslim : 0.004624529335485363\n",
      "sikh : 2.2161928735832024e-06\n",
      "temple : 2.216194719511279e-06\n",
      "ayodhya : 2.216174563435766e-06\n",
      "\n",
      "[Topic 2]\n",
      "hindu : 2.2083110791637973e-06\n",
      "muslim : 2.208398920276732e-06\n",
      "sikh : 2.208434508756428e-06\n",
      "temple : 2.20835559935458e-06\n",
      "ayodhya : 2.2083081329043313e-06\n",
      "\n",
      "[Topic 3]\n",
      "hindu : 2.3340599834284005e-06\n",
      "muslim : 2.334167342718934e-06\n",
      "sikh : 2.3340928214179743e-06\n",
      "temple : 2.334053802147635e-06\n",
      "ayodhya : 2.334051079747267e-06\n",
      "\n",
      "[Topic 4]\n",
      "hindu : 0.005796666027080499\n",
      "muslim : 2.3100642700551904e-06\n",
      "sikh : 2.3100408635802277e-06\n",
      "temple : 2.3100521151896305e-06\n",
      "ayodhya : 0.0045679768233735825\n",
      "\n",
      "[Topic 5]\n",
      "hindu : 2.2040054997802598e-06\n",
      "muslim : 2.2040872018022866e-06\n",
      "sikh : 2.2040642514366356e-06\n",
      "temple : 2.204035097352963e-06\n",
      "ayodhya : 2.204007464315639e-06\n",
      "\n",
      "[Topic 6]\n",
      "hindu : 2.104015662381107e-06\n",
      "muslim : 2.1040366081261875e-06\n",
      "sikh : 2.1041015969599117e-06\n",
      "temple : 2.1040111944820734e-06\n",
      "ayodhya : 2.104044823145903e-06\n",
      "\n",
      "[Topic 7]\n",
      "hindu : 2.3247921124597673e-06\n",
      "muslim : 2.324893448774435e-06\n",
      "sikh : 2.3249009141828203e-06\n",
      "temple : 2.3247863456650505e-06\n",
      "ayodhya : 2.3248288847320257e-06\n",
      "\n",
      "[Topic 8]\n",
      "hindu : 2.212858814811219e-06\n",
      "muslim : 2.212927492899955e-06\n",
      "sikh : 0.009213532944994286\n",
      "temple : 2.2128751376021938e-06\n",
      "ayodhya : 2.212847995981425e-06\n",
      "\n",
      "[Topic 9]\n",
      "hindu : 1.9745113496162807e-06\n",
      "muslim : 1.974599144028853e-06\n",
      "sikh : 1.9745651335953534e-06\n",
      "temple : 1.9744906587978814e-06\n",
      "ayodhya : 1.9745185960589294e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print religious word occurances\n",
    "words = list(headlines_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_headlines.components_])\n",
    "\n",
    "res = []\n",
    "for religious_word in religious_vocab:\n",
    "    if religious_word in words:\n",
    "        res.append(words.index(religious_word))\n",
    "#res = [words.index(religious_word) for religious_word in religious_vocab]\n",
    "\n",
    "#print('Displaying the top 10 words per topic and their probabilities within the topic...')\n",
    "#print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for i in res:\n",
    "        print(words[i], ':', topic_word_distributions[topic_idx, i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "\n",
    "for file in glob.glob('./india_headlines_data_pre/religious*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "religious_headlines_pre = pd.concat(li, axis = 0, ignore_index = True)\n",
    "\n",
    "li = []\n",
    "\n",
    "for file in glob.glob('./india_headlines_data_post/religious*.csv'):\n",
    "    data = pd.read_csv(file)\n",
    "    li.append(data)\n",
    "    \n",
    "religious_headlines_post = pd.concat(li, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process religious headlines\n",
    "processed_religious_headlines_pre = []\n",
    "\n",
    "for str_object in list(religious_headlines_pre[\"headline_text\"]):\n",
    "    processed_religious_headlines_pre.append(makeWordList(str_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use TfidfVectorizer to transform religious headlines\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=100, stop_words=\"english\", max_df=0.8)\n",
    "rel_headlines_pre_fit = vectorizer.fit(processed_religious_headlines_pre)\n",
    "X_rel_headlines_pre = vectorizer.fit_transform(processed_religious_headlines_pre).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_rel_headlines_pre = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda_rel_headlines_pre.fit(X_rel_headlines_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 10 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "gets : 0.3325878908343633\n",
      "vhp : 0.1410741139730405\n",
      "land : 0.09764653418812733\n",
      "rain : 0.07304272493430007\n",
      "women : 0.06716153659901956\n",
      "polls : 0.06155488657925728\n",
      "high : 0.06152229777309942\n",
      "churches : 0.04688881030527306\n",
      "hindus : 0.04190856687229721\n",
      "offer : 0.04053579900361241\n",
      "\n",
      "[Topic 1]\n",
      "monk : 0.09765610426785298\n",
      "babri : 0.08782965075859098\n",
      "gujarat : 0.08167533787612587\n",
      "centre : 0.07756836684415125\n",
      "rss : 0.0736208801311144\n",
      "day : 0.07045636329665927\n",
      "pakistan : 0.06295241149101867\n",
      "body : 0.05652299472458706\n",
      "modi : 0.056406034055761056\n",
      "today : 0.053136787481863235\n",
      "\n",
      "[Topic 2]\n",
      "body : 0.24650077884660038\n",
      "leader : 0.19598466160003897\n",
      "demand : 0.07515014473444322\n",
      "prayer : 0.07069654233678867\n",
      "court : 0.06340698526049049\n",
      "set : 0.0614349715701121\n",
      "christian : 0.05403263248965024\n",
      "new : 0.044979946185935954\n",
      "party : 0.038278258138742534\n",
      "state : 0.03787685008210023\n",
      "\n",
      "[Topic 3]\n",
      "asks : 0.31918242596499513\n",
      "meet : 0.18155867379434945\n",
      "arrested : 0.12690214788176327\n",
      "hc : 0.06845704101900134\n",
      "protest : 0.06276230618962748\n",
      "sikh : 0.06197819841034833\n",
      "quota : 0.04729651622342198\n",
      "verdict : 0.044561498859450016\n",
      "temple : 0.028898009056062598\n",
      "god : 0.028209089615466965\n",
      "\n",
      "[Topic 4]\n",
      "delhi : 0.5191071891445656\n",
      "cops : 0.09726389393121634\n",
      "govt : 0.07179057542441422\n",
      "demolition : 0.05059076099205142\n",
      "religion : 0.03841569682340742\n",
      "poll : 0.03815551890067723\n",
      "issue : 0.03256222521175726\n",
      "singh : 0.027293988099889054\n",
      "mark : 0.026936883948530464\n",
      "near : 0.022859895930397466\n",
      "\n",
      "[Topic 5]\n",
      "riots : 0.12714335344582472\n",
      "sikhs : 0.1091597834540954\n",
      "dead : 0.08626802724445033\n",
      "delhi : 0.08095105452303444\n",
      "devotees : 0.08062051573936894\n",
      "india : 0.07804703257980064\n",
      "security : 0.07257699013912884\n",
      "cong : 0.06746997136589813\n",
      "peace : 0.061250070601403694\n",
      "case : 0.06078109630459727\n",
      "\n",
      "[Topic 6]\n",
      "want : 0.40447147506028114\n",
      "say : 0.0901678825999094\n",
      "religious : 0.08322532336681733\n",
      "dont : 0.07560103232065452\n",
      "help : 0.06527663257809653\n",
      "support : 0.04824974585467745\n",
      "ayodhya : 0.04697002631844126\n",
      "divine : 0.037414784377852776\n",
      "group : 0.033118538046230495\n",
      "priest : 0.0305127680406536\n",
      "\n",
      "[Topic 7]\n",
      "imam : 0.32368457421577085\n",
      "puri : 0.21770924509320477\n",
      "city : 0.07559507109784995\n",
      "killed : 0.059702125466010614\n",
      "banaras : 0.04678099838199114\n",
      "row : 0.04083357629359221\n",
      "blast : 0.035956012082898337\n",
      "chief : 0.03229943102512417\n",
      "leaders : 0.03135421494323203\n",
      "plea : 0.028629309818073694\n",
      "\n",
      "[Topic 8]\n",
      "advani : 0.24871812901025805\n",
      "worship : 0.18081346887935246\n",
      "vote : 0.11925565033241153\n",
      "prayers : 0.08723915423169015\n",
      "seeks : 0.083063158499097\n",
      "hindu : 0.050612738086918435\n",
      "university : 0.039260665642189566\n",
      "police : 0.0345140988009695\n",
      "kerala : 0.030633094350126393\n",
      "gods : 0.029986469215317915\n",
      "\n",
      "[Topic 9]\n",
      "cm : 0.26090489426637037\n",
      "temples : 0.102805553110396\n",
      "begins : 0.0854785817970633\n",
      "cbi : 0.07729637331474787\n",
      "christians : 0.0769337609629199\n",
      "mosque : 0.07286794330711742\n",
      "attacks : 0.054484700966616555\n",
      "law : 0.049216810881430886\n",
      "says : 0.04002257059849445\n",
      "sc : 0.0373645219273359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = list(rel_headlines_pre_fit.vocabulary_)\n",
    "topic_word_distributions = np.array([row / row.sum() for row in lda_rel_headlines_pre.components_])\n",
    "num_top_words = 10\n",
    "\n",
    "print('Displaying the top 10 words per topic and their probabilities within the topic...')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(words[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
